{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b78601-9783-4e91-ba86-1ad132e7f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install libraries \n",
    "%pip install -q \"transformers>=4.45\" \"datasets>=2.20\" \"accelerate>=0.34\" \"trl>=0.9\" \"peft>=0.12\" bitsandbytes einops huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882772d1-afdf-422f-8eae-e1212613d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: Tesla V100-PCIE-16GB\n",
      "GPU 0: Tesla V100-PCIE-16GB (UUID: GPU-ae197c2c-be7d-4020-50e9-6dcdfa798789)\n",
      "\n",
      "Thu Oct  2 16:23:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           Off |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             24W /  250W |       4MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check GPU\n",
    "import torch, subprocess\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    try:\n",
    "        print(subprocess.check_output([\"nvidia-smi\",\"-L\"], text=True))\n",
    "        print(subprocess.check_output([\"nvidia-smi\"], text=True))\n",
    "    except Exception as e:\n",
    "        print(\"nvidia-smi not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226d85d8-bf30-488a-9fbc-68016f59ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"train.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae50e38-641c-4508-84b0-a4b2738719d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7260 prompt/response pairs\n",
      "{'messages': [{'content': 'Tell me the common excipient combinations for a Chewable tablet drug containing the active ingredient famotidine, calcium carbonate, magnesium hydroxide.', 'role': 'user'}, {'content': 'A Chewable tablet drug containing famotidine, calcium carbonate, magnesium hydroxide typically uses excipients such as anhydrous lactose, aspartame, dextrates, ferric oxide red, glyceryl monostearate, lactose monohydrate, magnesium stearate, microcrystalline cellulose, polysorbate 80, povidone, unspecified, talc.', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "#Parse prompt/response pairs and build a TRL chat dataset\n",
    "import re, random, json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = f.read()\n",
    "\n",
    "pair_re = re.compile(\n",
    "    r'Prompt:\\s*\"(?P<prompt>.*?)\"\\s*[\\r\\n]+Response:\\s*\"(?P<response>.*?)\"',\n",
    "    re.DOTALL | re.IGNORECASE\n",
    ")\n",
    "pairs = pair_re.findall(raw)\n",
    "print(f\"Found {len(pairs)} prompt/response pairs\")\n",
    "\n",
    "records = []\n",
    "for p, a in pairs:\n",
    "    records.append({\"messages\": [\n",
    "        {\"role\": \"user\", \"content\": p.strip()},\n",
    "        {\"role\": \"assistant\", \"content\": a.strip()},\n",
    "    ]})\n",
    "random.shuffle(records)\n",
    "\n",
    "dataset = Dataset.from_list(records)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1948ae3b-bbd3-4db8-99a1-cb6ad26f798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capability: (7, 0) bf16: False fp16: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "cc = torch.cuda.get_device_capability(0) if use_cuda else (0, 0)\n",
    "\n",
    "bf16_supported = False                 # V100 (7.0) doesn't support bf16\n",
    "fp16_supported = use_cuda              # yes\n",
    "\n",
    "print(\"Capability:\", cc, \"bf16:\", bf16_supported, \"fp16:\", fp16_supported)\n",
    "model_dtype = torch.float16 if fp16_supported else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ceb77fe-570e-420b-b1c0-e37d75b108e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",   # <-- important for V100\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "import os, gc\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "try:\n",
    "    base_model.config.use_cache = False\n",
    "except Exception:\n",
    "    pass\n",
    "gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ff4a6a-a976-443d-822d-7ddc2376a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"gemma3-1b-it-excipients-lora\",\n",
    "    per_device_train_batch_size=1,      \n",
    "    gradient_accumulation_steps=16,     \n",
    "    max_length=1024,\n",
    "    packing=True,\n",
    "    group_by_length=True,\n",
    "\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "\n",
    "    bf16=False,                        \n",
    "    fp16=True,                         \n",
    "    gradient_checkpointing=True,       \n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35150834-a805-4f6d-8bbf-1dedddf1de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define peft-config\n",
    "from peft import LoraConfig, get_peft_model\n",
    "peft_cfg = LoraConfig(\n",
    "    r=8,                     \n",
    "    lora_alpha=16,            \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46af78e3-2298-4eb6-b5ae-b8297c00487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908 908\n"
     ]
    }
   ],
   "source": [
    "import re, random\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_dataset_from_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    pair_re = re.compile(\n",
    "        r'Prompt:\\s*\"(?P<prompt>.*?)\"\\s*[\\r\\n]+Response:\\s*\"(?P<response>.*?)\"',\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    pairs = pair_re.findall(raw)\n",
    "\n",
    "    records = []\n",
    "    for p, a in pairs:\n",
    "        records.append({\"messages\": [\n",
    "            {\"role\": \"user\", \"content\": p.strip()},\n",
    "            {\"role\": \"assistant\", \"content\": a.strip()},\n",
    "        ]})\n",
    "    random.shuffle(records)\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "val_dataset   = load_dataset_from_txt(\"val.txt\")\n",
    "test_dataset  = load_dataset_from_txt(\"test.txt\")\n",
    "\n",
    "print(len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ce6bb6e-49b4-42ae-9096-ded410a8acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78373647e4b4637b03ba3dc92378102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/7260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc49b058b70b490583c20ad0b0267411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3693166babfb4de6bb17cd4d5bf42d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/7260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d675b005084a229e1c3d18ae8d502b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69e102cd4f14ba8aeb68310e039c104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae9f851f4164ff2a81d1662e3b0ce08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [205/205 25:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=205, training_loss=0.9156916967252405, metrics={'train_runtime': 1540.8579, 'train_samples_per_second': 2.109, 'train_steps_per_second': 0.133, 'total_flos': 1.382507412292224e+16, 'train_loss': 0.9156916967252405, 'entropy': 0.4992420447839273, 'num_tokens': 3271035.0, 'mean_token_accuracy': 0.8862284325264596, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fine-tune model on data \n",
    "from trl import SFTTrainer\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    # Convert the chat messages to a single string using the tokenizer’s chat template\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_cfg,\n",
    ")\n",
    "\n",
    "import gc, torch\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e93ad4ae-ecbb-47f1-bf70-ff094e6b7442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full model to: /group/sbms003/spinelli/models/gemma3-1b-it-excipients-finetuned\n"
     ]
    }
   ],
   "source": [
    "## Save full HF model \n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "\n",
    "ADAPTER_DIR = \"/group/sbms003/spinelli/models/gemma3-1b-it-excipients-lora/latest\"  #adapter folder\n",
    "OUT_DIR = \"/group/sbms003/spinelli/models/gemma3-1b-it-excipients-finetuned\"        #merged model folder\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(ADAPTER_DIR, device_map=\"cpu\")\n",
    "\n",
    "merged = peft_model.merge_and_unload()  \n",
    "\n",
    "merged.save_pretrained(OUT_DIR, safe_serialization=True) \n",
    "tok.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"Saved full model to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20529cb2-0ab9-4f84-9322-44f7841baab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Load Full Model \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "tok = AutoTokenizer.from_pretrained(OUT_DIR, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(OUT_DIR, device_map=\"auto\")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e6c08-50b9-4837-bf63-eba487d734a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Powder for suspension drug containing amoxicillin, clavulanic acid typically uses excipients such as methylparaben, sucrose, sucrose, water, carboxymethylcellulose sodium, sodium phosphate, tribasic, hypromellose, sucralose, fd&c red no. 40, fd&c red no. 40.\n"
     ]
    }
   ],
   "source": [
    "# After trainer.train()\n",
    "model = trainer.model.eval()  # PEFT-wrapped; already on device\n",
    "tok = tokenizer               # reuse your tokenizer\n",
    "\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me the common excipient combinations for a Powder for suspension drug containing the active ingredient amoxicillin, clavulanic acid.\"}\n",
    "]\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "out = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "response = out[0][\"generated_text\"][len(prompt):].strip()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a718c42f-fcb1-4ecc-9012-692d7a3a933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from rouge_score) (2.3.1)\n",
      "Requirement already satisfied: nltk in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from rouge_score) (2.2.6)\n",
      "Requirement already satisfied: six>=1.14.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from nltk->rouge_score) (8.3.0)\n",
      "Requirement already satisfied: joblib in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from nltk->rouge_score) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from nltk->rouge_score) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (2.2.6)\n",
      "Requirement already satisfied: dill in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (0.35.1)\n",
      "Requirement already satisfied: packaging in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: bert-score in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (2.2.6)\n",
      "Requirement already satisfied: dill in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (0.35.1)\n",
      "Requirement already satisfied: packaging in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from bert-score) (2.8.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from bert-score) (4.56.2)\n",
      "Requirement already satisfied: matplotlib in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from bert-score) (3.10.6)\n",
      "Requirement already satisfied: filelock in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from triton==3.4.0->torch>=1.0.0->bert-score) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from matplotlib->bert-score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from matplotlib->bert-score) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from matplotlib->bert-score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mmfs1/data/group/sbms003/spinelli/conda-envs/pytorch-gpu/lib/python3.10/site-packages (from matplotlib->bert-score) (3.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "!pip install evaluate\n",
    "!pip install evaluate bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "243c0681-4ff8-4d05-be69-0219bde388fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_length=512):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for example in dataset:\n",
    "        # Turn messages into a chat string using tokenizer’s template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc, labels=enc[\"input_ids\"])\n",
    "        losses.append(out.loss.item())\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(mean_loss)\n",
    "    return mean_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0412da6-652a-456b-b696-d6f87a1220ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7537, Perplexity: 2.12\n",
      "Test Loss: 0.7541, Perplexity: 2.13\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_ppl = compute_perplexity(trainer.model, tokenizer, val_dataset)\n",
    "test_loss, test_ppl = compute_perplexity(trainer.model, tokenizer, test_dataset)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {val_ppl:.2f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Perplexity: {test_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e6c34-a9e5-4102-99e0-fd677a87c078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c640543de841e59d4a00a5e95af412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87427bcc93b044caa22a7ee70253fd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93c9120a6246f7a9e471e3c0fa10af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1af85f61b44dd39b4b14ab3ed02fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d47d2877b7c41a398c4116310b2d087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " import evaluate\n",
    "\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def evaluate_generation(model, tokenizer, dataset, num_samples=100, max_new_tokens=150):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    # pick a subset (avoid evaluating on huge dataset at once)\n",
    "    subset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    for example in subset:\n",
    "        user_prompt = example[\"messages\"][0][\"content\"]\n",
    "        reference = example[\"messages\"][1][\"content\"]\n",
    "\n",
    "        # Prepare input\n",
    "        input_text = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": user_prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        prediction = tokenizer.decode(\n",
    "            gen[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        references.append(reference)\n",
    "\n",
    "    # Compute metrics\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "    bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu_score[\"bleu\"],\n",
    "        \"ROUGE-L\": rouge_score[\"rougeL\"],\n",
    "        \"BERTScore_F1\": sum(bert_score[\"f1\"]) / len(bert_score[\"f1\"])\n",
    "    }\n",
    "\n",
    "results = evaluate_generation(trainer.model, tokenizer, test_dataset, num_samples=100)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a65606-3a25-4c27-9105-873d95aa8d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
